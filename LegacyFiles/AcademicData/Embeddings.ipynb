{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv590/miniconda3/envs/chatbot-env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import ParentDocumentRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter, CharacterTextSplitter\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import HumanMessage\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from operator import itemgetter\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"f9d55f42-fec3-4cf8-b11a-1b4bc9812bbd\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"Salesforce/SFR-Embedding-Mistral\")\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discovering subpackages in _NamespacePath(['/Users/dhruv590/miniconda3/envs/chatbot-env/lib/python3.12/site-packages/pinecone_plugins'])\n",
      "Looking for plugins in pinecone_plugins.inference\n",
      "Installing plugin inference into Pinecone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': '', 'vectors': {}}\n",
      "record not exist\n"
     ]
    }
   ],
   "source": [
    "#vectorstore creation\n",
    "\"\"\"\n",
    "NOT USING FAISS FOR NOW ...\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents_list,embedding=embeddings)\n",
    "vectorstore.save_local(\"faiss_index\") #created index saved in RAG/model\n",
    "\n",
    "ENF OF COMMENT ...\n",
    "\"\"\"\n",
    "# New Implementation of vectorstore logic\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "import time\n",
    "\n",
    "index_name = \"course-catalog-scu\"\n",
    "index_dimension = 3072\n",
    "index_namespace = \"scu\"\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=index_dimension, \n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(\n",
    "            cloud=cloud, \n",
    "            region=region\n",
    "        ) \n",
    "    )\n",
    "\n",
    "while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "response = index.fetch(ids=[\"132b1c85-feef-44ea-a5b8-d6c178943c4\"])\n",
    "print(response)\n",
    "if not response[\"vectors\"]:\n",
    "    print(\"record not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Initialize a LangChain embedding object.\n",
    "model_name = \"multilingual-e5-large\" \n",
    "'''\n",
    "embeddings = PineconeEmbeddings(  \n",
    "    model=model_name,  \n",
    "    pinecone_api_key=os.environ.get(\"PINECONE_API_KEY\")  \n",
    ")\n",
    "''' \n",
    " \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings, namespace='scu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_self_query_retriever(vectorstore, top_k=10):\n",
    "    # Defining metadata fields\n",
    "    \n",
    "    metadata_field_info = [\n",
    "        AttributeInfo(\n",
    "            name=\"courseListing\",\n",
    "            description=\"The public listing of the course (e.g., 'ACTG 11A'), used in the course catalog and by students during registration.\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The official title of the course (e.g., 'intermediate financial accounting ii', 'business law for accountants'), used in the course catalog and on transcripts.\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"academicUnits\",\n",
    "            description=\"A list of academic departments or units responsible for the course (e.g., 'Accounting Department', 'Business Law Department', 'Computer Science Department').\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"schools\",\n",
    "            description=\"The school(s) under which the course is offered (e.g., 'Leavey School of Business', 'School of Engineering', 'School of Arts and Sciences').\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"courseSubjects\",\n",
    "            description=\"The subjects or topics covered by the course (e.g., 'Accounting', 'Business Law', 'Computer Science').\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"academicLevel\",\n",
    "            description=\"The academic level of the course, such as 'Undergraduate' or 'Graduate'.\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"prerequisiteCourses\",\n",
    "            description=\"A list of prerequisite courses that must be completed before enrolling in this course.\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"corequisiteCourses\",\n",
    "            description=\"A list of courses that must be taken concurrently with this course, if applicable.\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"specialTopics\",\n",
    "            description=\"Any special topics covered in the course that go beyond the standard curriculum.\",\n",
    "            type=\"list[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"publicNotes\",\n",
    "            description=\"Publicly available notes or comments about the course (e.g., specific enrollment restrictions or offerings).\",\n",
    "            type=\"Optional[string]\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"description\",\n",
    "            description=\"A detailed description of the course, including content, learning objectives, and any enrollment restrictions or prerequisites.\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"minimumUnits\",\n",
    "            description=\"The minimum number of units a student can earn by completing this course.\",\n",
    "            type=\"int\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"maximumUnits\",\n",
    "            description=\"The maximum number of units a student can earn by completing this course.\",\n",
    "            type=\"int\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"courseStatus\",\n",
    "            description=\"The current status of the course in the university system (e.g., 'Approved', 'Pending', 'Retired').\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Pre-retrieval metadata-tagged search using SelfQueryRetriever\n",
    "    document_content_description = \"chunks from a university database\"\n",
    "    llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "    \n",
    "    # Use SelfQueryRetriever for metadata-based search\n",
    "    meta_retriever = SelfQueryRetriever.from_llm(\n",
    "        llm,\n",
    "        vectorstore,  # Vector store to perform search\n",
    "        document_content_description,\n",
    "        metadata_field_info,\n",
    "        verbose=True,\n",
    "        search_kwargs={\"k\": top_k}\n",
    "    )\n",
    "    \n",
    "    return meta_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dynamic_combined_retriever(query, retriever):\n",
    "    # Perform the initial retrieval\n",
    "    initial_results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Extract scores of the retrieved documents that have a score attribute\n",
    "    scores = np.array([doc.score for doc in initial_results if hasattr(doc, 'score')])\n",
    "\n",
    "    # If scores exist, calculate the average score and standard deviation\n",
    "    if len(scores) > 0:\n",
    "        avg_score = np.mean(scores)\n",
    "        std_dev = np.std(scores)\n",
    "\n",
    "        # Use standard deviation to set a more dynamic threshold\n",
    "        dynamic_threshold = avg_score - (0.5 * std_dev)\n",
    "\n",
    "        # Filter results based on the dynamic threshold\n",
    "        filtered_results = [doc for doc in initial_results if hasattr(doc, 'score') and doc.score >= dynamic_threshold]\n",
    "\n",
    "        # Return filtered results and the original query\n",
    "        return filtered_results\n",
    "\n",
    "    # Return original results and the query if no scores are present\n",
    "    return initial_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Use pytorch device_name: mps\n",
      "Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "/Users/dhruv590/miniconda3/envs/chatbot-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pre-trained model for embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rerank_results_based_on_query(query, retrieved_docs, model):\n",
    "#     # Embed the query once\n",
    "#     query_embedding = model.encode(query, convert_to_tensor=True).unsqueeze(0)  # Shape: (1, embedding_dim)\n",
    "\n",
    "#     # Embed all documents at once, avoiding a loop\n",
    "#     doc_embeddings = model.encode([doc.page_content for doc in retrieved_docs], convert_to_tensor=True)\n",
    "\n",
    "#     # Compute cosine similarities in a vectorized way\n",
    "#     similarities = torch.nn.functional.cosine_similarity(query_embedding, doc_embeddings, dim=-1)\n",
    "    \n",
    "#     # Combine documents with their similarity scores\n",
    "#     doc_similarity_pairs = [(doc, similarity.item()) for doc, similarity in zip(retrieved_docs, similarities)]\n",
    "\n",
    "#     # Sort documents by similarity in descending order\n",
    "#     reranked_docs = sorted(doc_similarity_pairs, key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "#     return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results_based_on_query(query, retrieved_docs, model):\n",
    "    # Get the device (CPU or GPU)\n",
    "    device = torch.device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Embed the query once and move to the correct device\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True).unsqueeze(0).to(device)\n",
    "\n",
    "    # Embed all documents at once and move to the correct device\n",
    "    doc_embeddings = model.encode([doc.page_content for doc in retrieved_docs], convert_to_tensor=True).to(device)\n",
    "\n",
    "    # Compute cosine similarities in a vectorized way\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_embedding, doc_embeddings, dim=-1)\n",
    "\n",
    "    # Lower the query once for string matching\n",
    "    query_lower = query.lower()\n",
    "    print(query_lower)\n",
    "\n",
    "    # Prepare for boosting by pre-lowering metadata and boosting in vectorized manner\n",
    "    boosts = torch.ones(len(retrieved_docs), device=device)  # A tensor to hold boost factors on the correct device\n",
    "\n",
    "    # Iterate through documents to apply boosts\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        course_title = doc.metadata.get(\"course_title\", \"\").lower()  # Assuming metadata has course_title\n",
    "        course_listing = doc.metadata.get(\"course_listing\", \"\").lower()  # Assuming metadata has course_listing\n",
    "\n",
    "        # Apply boost if course title or course listing is found in the query\n",
    "        if course_title in query_lower or course_listing in query_lower:\n",
    "            boosts[i] = 1.5  # Boost factor\n",
    "\n",
    "    # Apply boosts to similarities\n",
    "    boosted_similarities = similarities * boosts\n",
    "\n",
    "    # Combine documents with their boosted similarity scores\n",
    "    doc_similarity_pairs = [(doc, boosted_similarity.item()) for doc, boosted_similarity in zip(retrieved_docs, boosted_similarities)]\n",
    "\n",
    "    # Sort documents by boosted similarity in descending order\n",
    "    reranked_docs = sorted(doc_similarity_pairs, key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_llm_output(doc_content, user_query):\n",
    "#     llm = ChatOpenAI(model_name='gpt-4', temperature=0.1)\n",
    "    \n",
    "#     prompt = f\"\"\"As a professional assistant for students, your task is to provide accurate and relevant information about courses based on the user's query.\n",
    "\n",
    "#                 User Query: {user_query}\n",
    "\n",
    "#                 You should only provide information about courses that explicitly match the user's query. Ignore any unrelated or vaguely related courses. The user is specifically asking about international accounting and financial reporting, so your response should only include courses that are **directly** related to those topics.\n",
    "\n",
    "#                 For each relevant course, provide:\n",
    "#                 1. Course code and title\n",
    "#                 2. A brief description\n",
    "#                 3. Any prerequisites or important notes\n",
    "\n",
    "#                 If no courses are directly relevant to the query, state that clearly.\n",
    "\n",
    "#                 Document Content: {doc_content}\n",
    "\n",
    "#                 Be precise and fact-based, and avoid providing information on courses that do not exactly match the user's query.\"\"\"\n",
    "    \n",
    "#     response = llm([HumanMessage(content=prompt)])\n",
    "    \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(doc_content):\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model_name='gpt-4', temperature=0.1)\n",
    "    \n",
    "    # Define the prompt with the improved professional message\n",
    "    prompt = (\n",
    "        f\"As a professional assistant for students, your goal is to provide accurate and reliable information. \"\n",
    "        f\"Based on the following document, craft a clear and useful summary or answer to the studentâ€™s question. \"\n",
    "        f\"only include the information that you think is relevant based on query and following document. \"\n",
    "        f\"Ensure that the response is precise, fact-based, and avoids any unnecessary assumptions or hallucinations. \"\n",
    "        f\"If there is missing or unclear information, only summarize what is available and suggest next steps if applicable. \"\n",
    "        f\"\\n\\nDocument Content: {doc_content}\"\n",
    "    )\n",
    "    \n",
    "    # Call the LLM using a HumanMessage format\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"tell me about accounting information systems\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the retriever and perform document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7h/45wztg294zj1__n6r50hl1mh0000gn/T/ipykernel_76440/4195230923.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  initial_results = retriever.get_relevant_documents(query)\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Generated Query: query='accounting information systems' filter=None limit=None\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "combined_retriever = advanced_self_query_retriever(vectorstore, top_k=10)\n",
    "\n",
    "# Perform dynamic retrieval and get the documents\n",
    "initial_results = dynamic_combined_retriever(user_query, combined_retriever)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerank the retrieved documents based on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank the retrieved documents based on the query\n",
    "reranked_documents = rerank_results_based_on_query(user_query, initial_results, model)\n",
    "# print(reranked_documents)\n",
    "for idx, doc in enumerate(reranked_documents):\n",
    "    print(f\"\\nDocument {idx + 1}:\")\n",
    "    print(f\"Source: {doc[0].metadata['source']}\")\n",
    "    print(f\"Relevance Score: {doc[1]}\")\n",
    "    print(f\"Content:\\n{doc[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LLM output based on top document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "top_5_documents = reranked_documents[:5]\n",
    "\n",
    "# Create a list to store the generated outputs\n",
    "generated_outputs = []\n",
    "\n",
    "# Function to process a document and generate output\n",
    "def process_document(doc_index, doc):\n",
    "    document_content = doc[0].page_content\n",
    "    llm_output = generate_llm_output(document_content)\n",
    "    return f\"Generated Output for Document {doc_index+1}:\\n{llm_output}\\n\"\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the calls\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks to the executor\n",
    "    future_to_doc = {executor.submit(process_document, i, doc): i for i, doc in enumerate(top_5_documents)}\n",
    "\n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(future_to_doc):\n",
    "        generated_outputs.append(future.result())\n",
    "\n",
    "# Now 'generated_outputs' will contain all the outputs, generated in parallel\n",
    "print(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the query and each of the generated outputs\n",
    "query_embedding = model.encode([user_query])\n",
    "output_embeddings = model.encode(generated_outputs)\n",
    "\n",
    "# Calculate cosine similarity between the query and each output\n",
    "similarities = cosine_similarity(query_embedding, output_embeddings)[0]\n",
    "\n",
    "# Rank outputs based on similarity\n",
    "ranked_outputs = sorted(zip(generated_outputs, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked outputs\n",
    "for rank, (output, score) in enumerate(ranked_outputs, start=1):\n",
    "    print(f\"Rank {rank} (Similarity: {score:.4f}): {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the document with the highest similarity score\n",
    "# top_document, top_similarity = reranked_documents[0]\n",
    "\n",
    "# # Pass the top document's content to the LLM for generating an output\n",
    "# llm_output = generate_llm_output(top_document.page_content)\n",
    "# print(f\"Generated Output: {llm_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
